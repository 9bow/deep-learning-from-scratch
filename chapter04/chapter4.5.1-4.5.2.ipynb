{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        \n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    # x: input, t: labeled\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1: (784, 100)\n",
      "b1: (100,)\n",
      "W2: (100, 10)\n",
      "b2: (10,)\n"
     ]
    }
   ],
   "source": [
    "net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "for k in net.params.keys():\n",
    "     print(\"{}: {}\".format(k, net.params[k].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.09922376  0.10399276  0.10001807  0.10083684  0.09511914  0.0996111\n",
      "   0.10010926  0.10483831  0.09634949  0.09990128]\n",
      " [ 0.09932867  0.10414784  0.09984373  0.10072486  0.09492603  0.09978659\n",
      "   0.10037575  0.10485692  0.09632202  0.0996876 ]\n",
      " [ 0.09937822  0.10383741  0.09950445  0.10090879  0.09500235  0.09980419\n",
      "   0.10023586  0.1050393   0.09640312  0.09988632]\n",
      " [ 0.09909568  0.10393555  0.09943899  0.10111711  0.09489182  0.09995432\n",
      "   0.10021687  0.10471837  0.09671256  0.09991872]\n",
      " [ 0.09909241  0.10395002  0.10014611  0.10088891  0.09497097  0.09954\n",
      "   0.10017637  0.10466547  0.09631165  0.1002581 ]\n",
      " [ 0.09955006  0.10401362  0.09977349  0.10072325  0.09505486  0.09972271\n",
      "   0.1001212   0.10475412  0.096475    0.09981169]\n",
      " [ 0.09913723  0.10397266  0.09955865  0.10084118  0.09510791  0.09960533\n",
      "   0.09997699  0.10545016  0.09645417  0.09989573]\n",
      " [ 0.09951554  0.1039435   0.10009295  0.10045854  0.09479408  0.0997051\n",
      "   0.10041437  0.10485246  0.09633911  0.09988435]\n",
      " [ 0.09900119  0.10404764  0.09988195  0.10087761  0.09525287  0.09976472\n",
      "   0.1002256   0.10488473  0.09637762  0.09968606]\n",
      " [ 0.09921487  0.10398426  0.0997139   0.10088451  0.09501391  0.10005201\n",
      "   0.10039514  0.10485753  0.09624988  0.09963399]\n",
      " [ 0.09960335  0.10391376  0.09958826  0.10074488  0.09504742  0.10002737\n",
      "   0.10012127  0.10478223  0.09675524  0.09941623]\n",
      " [ 0.09912579  0.10389014  0.09965279  0.10102588  0.09485211  0.10035116\n",
      "   0.09998129  0.10496281  0.09614095  0.10001706]\n",
      " [ 0.09909674  0.10374989  0.09975206  0.10055435  0.09510208  0.10013278\n",
      "   0.10016158  0.10467962  0.09635599  0.10041489]\n",
      " [ 0.09945335  0.10398862  0.09925328  0.10076     0.09525195  0.09978638\n",
      "   0.10005366  0.10517171  0.09659354  0.09968752]\n",
      " [ 0.09917121  0.10398652  0.09971824  0.10127304  0.09502401  0.10006952\n",
      "   0.09992216  0.10481734  0.09623192  0.09978604]\n",
      " [ 0.09917822  0.10435616  0.09965714  0.10041978  0.09506906  0.09995005\n",
      "   0.09996172  0.10485043  0.09663622  0.09992121]\n",
      " [ 0.09968939  0.10386854  0.09986738  0.10086751  0.09493674  0.09959356\n",
      "   0.09993184  0.10488164  0.09643623  0.09992719]\n",
      " [ 0.0992406   0.10406381  0.09920357  0.10070558  0.094892    0.10000242\n",
      "   0.10026978  0.10515211  0.09662625  0.09984388]\n",
      " [ 0.09918552  0.10416968  0.09965767  0.10051998  0.09514554  0.09985174\n",
      "   0.10022153  0.10501246  0.0964465   0.09978939]\n",
      " [ 0.09917617  0.10435719  0.09987866  0.10068754  0.09517876  0.09955895\n",
      "   0.10031591  0.1045636   0.09663973  0.0996435 ]\n",
      " [ 0.09930411  0.10421784  0.09956768  0.10081824  0.09503628  0.10004337\n",
      "   0.10018285  0.10482724  0.09617952  0.09982288]\n",
      " [ 0.0994775   0.10428717  0.09998826  0.10032767  0.09505402  0.09977789\n",
      "   0.1001856   0.10467279  0.09642069  0.09980842]\n",
      " [ 0.09895276  0.10386733  0.09965004  0.10140456  0.09517874  0.09979261\n",
      "   0.09996255  0.10504894  0.09625377  0.0998887 ]\n",
      " [ 0.09926924  0.10360389  0.0995562   0.10096506  0.09487989  0.10020932\n",
      "   0.10010124  0.10495395  0.09644883  0.10001238]\n",
      " [ 0.09904887  0.1040946   0.09969751  0.1006494   0.09490369  0.09996538\n",
      "   0.10023648  0.10491327  0.09636891  0.10012189]\n",
      " [ 0.09895109  0.10409492  0.09998918  0.10084611  0.09482766  0.09998367\n",
      "   0.10016773  0.10468567  0.09642629  0.10002768]\n",
      " [ 0.09909019  0.10390238  0.09960781  0.10042576  0.09540283  0.10009328\n",
      "   0.10005233  0.10493323  0.09672422  0.09976796]\n",
      " [ 0.09944927  0.1040357   0.09984532  0.10052964  0.09514516  0.09939307\n",
      "   0.10001706  0.10531293  0.09634344  0.0999284 ]\n",
      " [ 0.0992346   0.10413782  0.09986475  0.10056357  0.09481949  0.09994998\n",
      "   0.10005977  0.10476672  0.09642975  0.10017355]\n",
      " [ 0.09919213  0.1038546   0.09958267  0.100948    0.09513396  0.0999388\n",
      "   0.10017012  0.10482044  0.09640584  0.09995345]\n",
      " [ 0.09917523  0.1039139   0.09970495  0.10072595  0.09524536  0.09977743\n",
      "   0.10020575  0.10481336  0.0966344   0.09980367]\n",
      " [ 0.09915028  0.10414504  0.09967597  0.10096672  0.09495878  0.09990802\n",
      "   0.09982979  0.10509787  0.09659034  0.09967719]\n",
      " [ 0.0993254   0.10366142  0.09976706  0.10113028  0.09470999  0.09997017\n",
      "   0.10006859  0.10525909  0.09596971  0.10013829]\n",
      " [ 0.09894516  0.1039649   0.09976284  0.10089885  0.09503204  0.09988719\n",
      "   0.10014396  0.10486139  0.09628563  0.10021804]\n",
      " [ 0.09896278  0.10415517  0.09991796  0.10112952  0.09484074  0.10006964\n",
      "   0.10002734  0.10480531  0.09617144  0.0999201 ]\n",
      " [ 0.09934309  0.10400542  0.09975388  0.10096689  0.09503529  0.09981076\n",
      "   0.10002658  0.10508634  0.09618962  0.09978213]\n",
      " [ 0.09916654  0.10421229  0.09971002  0.10088207  0.0946369   0.09999662\n",
      "   0.1001609   0.10493291  0.09635093  0.0999508 ]\n",
      " [ 0.0994172   0.10394087  0.09973545  0.10075567  0.09495971  0.09971828\n",
      "   0.10041184  0.10469779  0.09656023  0.09980296]\n",
      " [ 0.09956831  0.10410107  0.10003081  0.10051648  0.09499472  0.09947642\n",
      "   0.10034878  0.10470891  0.09656222  0.0996923 ]\n",
      " [ 0.09909277  0.10402646  0.09969067  0.1012587   0.09479245  0.100034\n",
      "   0.09994326  0.10484159  0.09614223  0.10017788]\n",
      " [ 0.09923507  0.10419574  0.09992818  0.10060758  0.09481703  0.09960698\n",
      "   0.10035471  0.10468048  0.0964005   0.10017372]\n",
      " [ 0.09913369  0.10409025  0.09991267  0.10102853  0.09504177  0.10016608\n",
      "   0.09987498  0.10479149  0.09646707  0.09949346]\n",
      " [ 0.09932194  0.10374051  0.09954108  0.10054349  0.09518377  0.09971265\n",
      "   0.10007054  0.10522933  0.09655491  0.10010177]\n",
      " [ 0.09962666  0.10399969  0.09978975  0.10095092  0.09496519  0.09971966\n",
      "   0.10008583  0.10495845  0.09643956  0.0994643 ]\n",
      " [ 0.09962951  0.10386342  0.09972498  0.10082889  0.09475401  0.10004861\n",
      "   0.10011048  0.10494538  0.0962946   0.09980011]\n",
      " [ 0.0991819   0.1037511   0.09979768  0.10113504  0.09539032  0.09992684\n",
      "   0.10011313  0.10492358  0.09640679  0.09937362]\n",
      " [ 0.09924173  0.10412444  0.09979539  0.10089869  0.09515628  0.09975156\n",
      "   0.10004858  0.10475225  0.09655626  0.09967481]\n",
      " [ 0.09915053  0.10374284  0.09946982  0.10087784  0.09478775  0.09981142\n",
      "   0.10023813  0.10521199  0.09688195  0.09982773]\n",
      " [ 0.09930345  0.10430171  0.0997728   0.10109092  0.09488775  0.09987349\n",
      "   0.10003281  0.10434162  0.09637727  0.10001818]\n",
      " [ 0.09934893  0.10396647  0.09973368  0.1010258   0.09533509  0.09962189\n",
      "   0.09997432  0.10502285  0.09640017  0.09957079]\n",
      " [ 0.09911894  0.10384369  0.09996841  0.10073862  0.0949577   0.0998546\n",
      "   0.1002464   0.10501369  0.09615972  0.10009822]\n",
      " [ 0.09941199  0.1040872   0.0997288   0.10080735  0.09510922  0.09990611\n",
      "   0.09986894  0.1050809   0.09648707  0.09951243]\n",
      " [ 0.0988907   0.10394021  0.09968099  0.1010877   0.09516883  0.09983806\n",
      "   0.10027768  0.10478495  0.09649151  0.09983937]\n",
      " [ 0.09925043  0.10385799  0.10014699  0.10069085  0.0951827   0.09987485\n",
      "   0.10004535  0.10502542  0.0965311   0.09939432]\n",
      " [ 0.09921038  0.10412365  0.0997946   0.10061505  0.09511636  0.09981798\n",
      "   0.09996995  0.10524954  0.09636225  0.09974025]\n",
      " [ 0.09928777  0.10401777  0.09988354  0.10106679  0.09503999  0.09974422\n",
      "   0.09982334  0.10478075  0.09655459  0.09980125]\n",
      " [ 0.09936603  0.1037432   0.09956181  0.10080974  0.09507886  0.09987547\n",
      "   0.10020729  0.10475027  0.09685381  0.09975351]\n",
      " [ 0.09903582  0.10429394  0.09983032  0.10116634  0.09469791  0.09986946\n",
      "   0.09992943  0.10520473  0.09631895  0.09965312]\n",
      " [ 0.09920918  0.10395615  0.09978924  0.10100901  0.09512893  0.09975451\n",
      "   0.09992345  0.10484679  0.09665663  0.09972611]\n",
      " [ 0.09958339  0.10364141  0.09961283  0.10053153  0.09512916  0.09979097\n",
      "   0.10013694  0.10526726  0.09654507  0.09976144]\n",
      " [ 0.09914117  0.10397038  0.0997108   0.10075144  0.09530935  0.09986447\n",
      "   0.0998341   0.10511771  0.09637451  0.09992608]\n",
      " [ 0.09884715  0.10456156  0.09971486  0.10094197  0.09496156  0.09981518\n",
      "   0.09999078  0.10486343  0.09635289  0.09995062]\n",
      " [ 0.09945721  0.10402064  0.09995781  0.10027879  0.0953032   0.09981184\n",
      "   0.10006395  0.10486121  0.09636896  0.09987639]\n",
      " [ 0.09913458  0.10387974  0.09983084  0.10093494  0.0951376   0.09983525\n",
      "   0.09997575  0.10490414  0.0962163   0.10015087]\n",
      " [ 0.09925293  0.10410035  0.09986073  0.10049536  0.09499484  0.09989952\n",
      "   0.10024233  0.10471521  0.09640795  0.10003078]\n",
      " [ 0.09909031  0.10397396  0.09981278  0.10061003  0.09509382  0.09994223\n",
      "   0.10021508  0.10465707  0.09643642  0.1001683 ]\n",
      " [ 0.09920327  0.10407478  0.09987526  0.10059994  0.0950785   0.09982921\n",
      "   0.10007509  0.10478689  0.09676995  0.09970712]\n",
      " [ 0.09943836  0.10401034  0.09976471  0.10116977  0.09483492  0.09980212\n",
      "   0.09976946  0.10502186  0.09634391  0.09984456]\n",
      " [ 0.09920825  0.10391982  0.09974468  0.10093238  0.09513013  0.09990202\n",
      "   0.10021946  0.1047115   0.09652829  0.09970347]\n",
      " [ 0.09918395  0.10403268  0.09992354  0.10089692  0.09498518  0.09993645\n",
      "   0.09994814  0.10480928  0.09647632  0.09980753]\n",
      " [ 0.09915895  0.10403082  0.09947484  0.10121079  0.09555842  0.09937867\n",
      "   0.10005448  0.1050243   0.09609585  0.10001289]\n",
      " [ 0.09943191  0.10378685  0.09967746  0.1004744   0.09531277  0.0994842\n",
      "   0.1002218   0.10537476  0.09651356  0.09972228]\n",
      " [ 0.09960496  0.10397254  0.09994432  0.10048627  0.09467207  0.09977464\n",
      "   0.09999805  0.10488748  0.09660403  0.10005563]\n",
      " [ 0.09911029  0.1038785   0.10004873  0.10109006  0.09478403  0.09992404\n",
      "   0.10008732  0.10499965  0.09637438  0.09970301]\n",
      " [ 0.09934176  0.10395048  0.09940836  0.10091054  0.09515555  0.09995282\n",
      "   0.10025045  0.10463378  0.0964315   0.09996478]\n",
      " [ 0.09944687  0.10371689  0.09959681  0.1006911   0.0948172   0.09968644\n",
      "   0.10041901  0.10488969  0.0964258   0.10031019]\n",
      " [ 0.09913417  0.10409633  0.09954858  0.10090897  0.09482308  0.09971299\n",
      "   0.10029215  0.10518308  0.0963129   0.09998775]\n",
      " [ 0.09915394  0.10396196  0.10013968  0.10100239  0.09484779  0.09995646\n",
      "   0.10002321  0.10466181  0.0962123   0.10004046]\n",
      " [ 0.09946705  0.10430415  0.09975667  0.10027812  0.09498044  0.09964315\n",
      "   0.10032627  0.1050595   0.09647898  0.09970567]\n",
      " [ 0.09924611  0.10384915  0.09987464  0.10053435  0.09518281  0.0999979\n",
      "   0.10021213  0.1047726   0.09630354  0.10002677]\n",
      " [ 0.09926528  0.10374548  0.09987482  0.1006531   0.09499564  0.09971126\n",
      "   0.10016282  0.10499176  0.09655253  0.10004731]\n",
      " [ 0.0990759   0.10391262  0.09969021  0.10107053  0.09516041  0.09992426\n",
      "   0.09984878  0.10504072  0.09629385  0.09998274]\n",
      " [ 0.09934626  0.10427042  0.09985974  0.10069364  0.09475778  0.09979462\n",
      "   0.1002422   0.1047368   0.09644293  0.09985561]\n",
      " [ 0.09882193  0.10392733  0.10000535  0.10124286  0.09502404  0.10008771\n",
      "   0.10004244  0.10489216  0.0962767   0.09967947]\n",
      " [ 0.09985484  0.10417325  0.09957464  0.10063043  0.0952955   0.09967153\n",
      "   0.10027707  0.10455369  0.09636273  0.09960633]\n",
      " [ 0.09919081  0.10390325  0.09989397  0.10058129  0.09489924  0.09994906\n",
      "   0.10001638  0.10509516  0.09665164  0.0998192 ]\n",
      " [ 0.0993329   0.10378296  0.0998278   0.1005918   0.09486528  0.09986745\n",
      "   0.10022586  0.10485258  0.09642499  0.10022837]\n",
      " [ 0.09936566  0.10412966  0.0997885   0.10089727  0.09485309  0.09987179\n",
      "   0.10000004  0.10518686  0.09642468  0.09948245]\n",
      " [ 0.09913604  0.10435671  0.09944736  0.10092799  0.09499304  0.09980378\n",
      "   0.10025657  0.10470365  0.09618746  0.10018741]\n",
      " [ 0.09881767  0.10371371  0.09977994  0.10115002  0.09496657  0.09998107\n",
      "   0.10046957  0.10486886  0.09647712  0.09977548]\n",
      " [ 0.09895302  0.10376664  0.09968059  0.10090202  0.0952194   0.10008462\n",
      "   0.10024095  0.10497278  0.09638895  0.09979104]\n",
      " [ 0.0992392   0.10397941  0.10008618  0.1009585   0.09449207  0.10000785\n",
      "   0.10008866  0.10451961  0.09656964  0.10005888]\n",
      " [ 0.09926693  0.10394698  0.09970036  0.1012416   0.09533975  0.10010834\n",
      "   0.09988498  0.10471723  0.09610792  0.09968589]\n",
      " [ 0.09894558  0.10417541  0.09985686  0.10100016  0.09514427  0.09987515\n",
      "   0.1002462   0.10487874  0.09611605  0.09976158]\n",
      " [ 0.09930501  0.10373813  0.09975618  0.10130807  0.09509474  0.09985813\n",
      "   0.10019481  0.1047534   0.09608163  0.09990991]\n",
      " [ 0.09904929  0.10401963  0.09950416  0.10064068  0.09500512  0.09995413\n",
      "   0.10032778  0.10514314  0.09642526  0.09993082]\n",
      " [ 0.09892075  0.10399678  0.09971184  0.10094952  0.09510177  0.10020748\n",
      "   0.10000246  0.10495457  0.09627477  0.09988005]\n",
      " [ 0.09939146  0.10352829  0.0995281   0.10094491  0.09517478  0.09973444\n",
      "   0.1003886   0.10474961  0.09653733  0.10002248]\n",
      " [ 0.09929064  0.1039173   0.09981723  0.10043878  0.09503059  0.09991642\n",
      "   0.10017135  0.10452274  0.09706077  0.09983417]\n",
      " [ 0.09927605  0.10403283  0.09983759  0.10065822  0.09506244  0.09955629\n",
      "   0.10021743  0.10488909  0.09652963  0.09994043]]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(100, 784)\n",
    "y = net.predict(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.random.rand(100, 784)\n",
    "t = np.random.rand(100, 10)\n",
    "\n",
    "grads = net.numerical_gradient(x, t)\n",
    "\n",
    "for k in grads.keys():\n",
    "     print(\"{}: {}\".format(k, grads[k].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "train_loss_list = []\n",
    "\n",
    "# Hyper Parameters\n",
    "iters_num = 10000 # no. of iteration\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100 # size of mini-batch\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# train\n",
    "for i in range(iters_num):\n",
    "    # get mini-batch\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # calc. gradient\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    # grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # update params\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # loss\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        print(\"{}: {}\".format(i, loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
